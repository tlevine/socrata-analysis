---
title: 
tags: ['open-data']
---
```{r configure, echo = F}
source('15-dates.r')
opts_chunk$set(fig.width=10, fig.height=10, dpi = 42 * 5)
```

## Data portal statistics
According to A, B, C and D, getting data to the web, in whatever form
you can, is always better than nothing. How can you tell whether you're
doing a good job at this basic release of data? Data catalog software
typically provides a tool for this: a statistic of the number of datasets. We can go to
[data.gov.uk](https://data.gov.uk) and find out that it currently has
however many datasets, and we can go back next week and make sure that
the number is increasing. There are problems with this measure, which I
won't go into right now, but the measure does tell us something about how
much data we are releasing.

According to A, B, C and D, it is also important that we keep open data
up-to-date. Unfortunately, data catalog software don't currently
have great tools for helping you make sure that your data are indeed
up-to-date. Specifically, we don't have a number that we can look at
week-to-week to make sure that we are vaguely on track at keeping data
up-to-date. So I made one.

## The data about data
I [downloaded]()
all the metadata about all of the public datasets, and then I removed
datasets that were [duplicated because of federation]().

Socrata has this feature where you can create multiple
[views]() from a single
[table]().
It is hard to separate the official views, created by someone in the
government, from the unofficial, derived views. I did not try to determine
which view/dataset was the original, official one, but I did separate
them based on their `tableId`, which links all of the different views
that use the same data.

The Socrata API provides four date fields.
* `createdAt`
* `publicationDate`
* `rowsUpdatedAt`
* `viewLastModified`

`createdAt` is when someone clicked the button to include the dataset;
it's always the oldest one. It is possible to create a dataset but keep
it private, for internal review. If you spend a couple days reviewing
the dataset privately before you post it publically, the `publicationDate`
will reflect this latter date, while `createdAt` will still be the time
when you added the private dataset. Also, `createdAt` is different for
different views of the same data, but `publicationDate` is not.

```{r publicationDate}
table(subset(socrata.deduplicated, tableId == 926641)$publicationDate)
```

```{r createdAt}
table(subset(socrata.deduplicated, tableId == 926641)$createdAt)
```

`rowsUpdatedAt` and `viewLastModified` presumably have something to do
with the time the dataset was updated. I don't really know what those
mean, but I think `rowsUpdatedAt` is the one I want. Recall from above
that views with the same `tableId` share the share the underlying data.
If the data get updated, it should be reflected in all of the datasets.
This is the case for `rowsUpdatedAt`; for all
`r sum(socrata.deduplicated.orig$tableId == 926641)`
datasets with an `tableId` of `926641`, the `rowsUpdatedAt` field
is the same.

```{r rowsUpdatedAt}
table(subset(socrata.deduplicated.orig, tableId == 926641)$rowsUpdatedAt)
```

On the other hand, the `viewLastModified` field is quite different.

```{r viewLastModified}
table(subset(socrata.deduplicated.orig, tableId == 926641)$viewLastModified)
```

Thus, I'm using `rowsUpdatedAt` for calculating the updatedness measure.

## Data summary
Let's start by plotting these date variables by dataset.
I also included the publication group number, which is presumably
an identification number that increases with each group of dataset
publication.

```{r up_to_date}
p1
```

It looks like most data were "updated" only at their time of their
initial publication. Colloquially, we'd say that they haven't been
updated ever. But there are some spikes here and there.

This is a bit complicated; let's try to come up with simpler measure
for taking this in.

## Motivation for the measure
One measure that I came up with is **proportion of datasets older than
a year that have been updated within the year**. Here's how I arrived
at that measure.

### General formulation
In producing a measure of updateness, we are trying to come up with a
simple number that matches our intuition about whether something is
up-to-date. Let's think a bit about what would count as up-to-date.

A dataset first gets published at some time. For a little while, the
data will be up-to-date even if no updates are sent. For example, if
data about wifi usage per month are uploaded on the first of the month,
we'll consider them to be up-to-date two weeks later.

After some point, we must have received an update in order for the
data to be up-to-date. In the case of monthly wifi usage, we need an
update on the first of the second month; if we haven't received one
by then, we'll consider the data out-of-date. Thus, I arrive at the
general concept of comparing the date of initial publication to the
date at which the dataset was last updated.

### Differences by dataset
If a dataset of scores on standardized math tests taken in schools
was uploaded two weeks ago and hasn't been touched since, I'd say it's
up-to-date. A particular standardized math tests might be administered
yearly, so I don't expect the data to have changed in a couple of weeks.

On the other hand, if a dataset of 311 calls hasn't been updated in
two weeks, we could say that it is out-of-date because 311 calls are
always coming in.

A dataset about something that happens often needs to be refreshed
more often for us to consider it up-to-date. A very cool measure of
updatedness would account for out how often new data come in and
check whether those new data show up in the dataset. But that would
be a lot of work, so let's start with something simpler.

### Keep it simple
Rather than worring about the exact time period, let's address a simpler
issue. I've heard complaints that some data get uploaded data once and
never updated, ever. Rather than worring about the exact time since update,
let's just check whether the dataset has been updated ever.

I'm going to use the format of *datasets older than X that have been updated since*
and just choose an appropriate time X. The time probably has to be kind of
long, so that we don't count datasets as out-of-date when they just have
long update cycles (like yearly data). On the other hand, the time can't be
too long, because then we'll wind up saying that everything is out-of-date.

### Choosing the specific value
We can check whether different values will give us the same value on our
updatedness measure. To say it fancy-like, we can check whether our updatedness
measure is robust to the cutoff.

A simple way of checking this is to plot the value of our updatedness
measure for different cutoff values. If we find a region where the value
of the updatedness measure doesn't change very much when we change the
cutoff, we could say that cutoffs in that region are safe to use.

I also had to decide whether to use `rowsUpdatedAt` or `viewLastModified`
as the date of update. I don't really know what either of these means, so
let's just try both.

```{r robustness}
p3
```

The main thing I notice is that the value of this statistic
(again, proportion of datasets older than some date that have been updated since)
is pretty close to zero for all of the portals at any time.
This is very bad; it means that we pretty much never update
data that we put on data portals.

The next thing I notice is that the value varies widely when there aren't
very many datasets on the portal. Visually, this shows itself as skinny
lines (few datasets) jumping up and down very suddenly, like for
`data.cms.gov`. When there are very few datasets on a portal
(like 12), an addition of a dataset or an update to a dataset will have
a large impact on the value of our updatedness measure (the height of the line).
This is not interesting, and the plot below should explain why.

```{r small_samples}
p4
```

In these two plots, we're only looking at `data.cms.gov`. The first
plot is the same as the corresponding plot in the previous image,
only larger. The second shows the dates at which each
[data table](/!/) was
published and most recently updated. Red dots are publication dates,
and blue dots are update dates. If there was no update or the most
recent update was within a day of the publication date, no blue dot
is shown.

The bumps in the top plot only occur when the portal contains very
few datasets; the figure stabilizes at the end of March 2013, when
a bunch of datasets get published. (In case you're curious, these
datasets are all derived from the
[Beneficiary Summary File](http://www.resdac.org/cms-data/files/bsf),
according to their respective descriptions in the portal.)

## Even simpler
I'd assumed that people actually update their data, but it looks
like people don't.
I think we need something simpler to drive this conclusion home.

Let's look at the number of datasets that have ever been updated, by
portal. We'll count a dataset as having been updated if it has a value
in `rowsUpdatedAt` that is more than one hour greater than the value
in `publicationDate`.


```{r small_samples}
p4
```

It is quite likely that people have "updated" a dataset by uploading
a new version. For example, XXX uploading 2010 data
[here]()
and 2011 data
[here]().
But this is less helpful than updating the original dataset because
you need to find the new dataset, make sure that it is indeed just
a new version of the same dataset, and update your software to point
to both datasets.


## The bigger picture
I thought this was going to be an article about a measure for how up-to-date
our data portal data are, but I wound up finding that the data are completely
out-of-date. Given the hype 

Maybe this says something about the need for more measures of the progress of
our open data efforts; nobody was measuring updatedness, so how could anyone
have known to keep the data up-to-date?





Local law 11
March 18
September something
