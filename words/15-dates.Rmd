---
title: How up-to-date are data portal datasets?
tags: ['open-data']
---
```{r configure, echo = F}
source('15-dates.r')
opts_chunk$set(fig.width=10, fig.height=10, dpi = 42 * 5)
```

```{r up_to_date}
p1
```

## Data portal metrics
According to A, B, C and D, getting data to the web, in whatever form
you can, is always better than nothing. How can you tell whether you're
doing a good job at this basic release of data? Data catalog software
typically provides one tool: the metric of number of datasets. We can go to
[data.gov.uk](https://data.gov.uk) and find out that it currently has
however many datasets, and we can go back next week and make sure that
the number is increasing. There are problems with this metric, which I
won't go into right now, but the metric does tell us something about how
much data we are releasing.

According to A, B, C and D, it is also important that we keep open data
up-to-date. Unfortunately, data catalog software don't currently
have great tools for helping you make sure that your data are indeed
up-to-date. Specifically, we don't have a number that we can look at
week-to-week to make sure that we are vaguely on track at keeping data
up-to-date. So I made one.

## The data about data
The Socrata API provides four date fields.
* `createdAt`
* `publicationDate`
* `rowsUpdatedAt`
* `viewLastModified`

`createdAt` is when someone clicked the button to include the dataset;
it's always the oldest one. It is possible to create a dataset but keep
it private, for internal review. If you spend a couple days reviewing
the dataset privately before you post it publically, the `publicationDate`
will reflect this latter date, while `createdAt` will still be the time
when you added the private dataset.

`rowsUpdatedAt` and `viewLastModified` presumably have something to do
with the time the dataset was updated. I don't really know what those
mean.

federation and non-tables removed. but that doesn't work. i've explained this somewhere
XXX

### Problems with these data about data
As I indicate above, I don't really understand what the data I'm using
mean. Specifically, I don't really know what `rowsUpdatedAt` and
`viewLastModified` mean, and there are issues with the deduplication
of datasets. Thus, it's better to see this article as a prototype of a
metric rather than a metric that we should really be using or an analysis
that really tells us something.

## Motivation for the metric
The metric that I came up with is **proportion of datasets older than
a year that have been updated within the year**. Here's how I arrived
at that metric.

### General formulation
In producing a metric of updateness, we are trying to come up with a
simple number that matches our intuition about whether something is
up-to-date. Let's think a bit about what would count as up-to-date.

A dataset first gets published at some time. For a little while, the
data will be up-to-date even if no updates are sent. For example, if
data about wifi usage per month are uploaded on the first of the month,
we'll consider them to be up-to-date two weeks later.

After some point, we must have received an update in order for the
data to be up-to-date. In the case of monthly wifi usage, we need an
update on the first of the second month; if we haven't received one
by then, we'll consider the data out-of-date. Thus, I arrive at the
general concept of comparing the date of initial publication to the
date at which the dataset was last updated.

### Differences by dataset
If a dataset of scores on standardized math tests taken in schools
was uploaded two weeks ago and hasn't been touched since, I'd say it's
up-to-date. A particular standardized math tests might be administered
yearly, so I don't expect the data to have changed in a couple of weeks.

On the other hand, if a dataset of 311 calls hasn't been updated in
two weeks, we could say that it is out-of-date because 311 calls are
always coming in.

A dataset about something that happens often needs to be refreshed
more often for us to consider it up-to-date. A very cool metric of
updatedness would account for out how often new data come in and
check whether those new data show up in the dataset. But that would
be a lot of work, so let's start with something simpler.

### All or nothing
Rather than worring about the exact time period, let's address a simpler
issue. I've heard complaints that some data get uploaded data once and
never updated, ever. Rather than worring about the exact time since update,
let's just check whether the dataset has been updated ever.

I'm going to use the format of *datasets older than X that have been updated since*
and just choose an appropriate time X. The time probably has to be kind of
long, so that we don't count datasets as out-of-date when they just have
long update cycles (like yearly data). On the other hand, the time can't be
too long, because then we'll wind up saying that everything is out-of-date.

### Choosing the specific value
We can check whether different values will give us the same value on our
updatedness metric. To say it fancy-like, we can check whether our updatedness
metric is robust to the cutoff.

A simple way of checking this is to plot the value of our updatedness
metric for different cutoff values. If we find a region where the value
of the updatedness metric doesn't change very much when we change the
cutoff, we could say that cutoffs in that region are safe to use.

I also had to decide whether to use `rowsUpdatedAt` or `viewLastModified`
as the date of update. I don't really know what either of these means, so
let's just try both.

```{r robustness}
p4
```

The first thing I notice when I look at this plot is that the metric based
on `rowsUpdatedAt` (labeled "rows") is pretty much always zero.
If `rowsUpdatedAt` means what I intuitively think of as the date of an update
to the data, this is very bad; it means that we pretty much never update
data that we put on data portals. On the other hand, the metric based on
`viewLastModified` ("view") sometimes gets quite high, so let's hope that
that measure tells us something.

The next thing I notice is that the value varies widely when there aren't
very many datasets on the portal. Visually, this shows itself as skinny
lines (few datasets) that are blue (based on `viewLastModified`) jumping
up and down very suddenly. When there are very few datasets on a portal
(like 12), an addition of a dataset or an update to a dataset will have
a large impact on the value of our updatedness metric (the height of the line).

Let's look at just the plots for data portals that have had lots of datasets
for quite a long time.

```{r robustness_big_portals}
p5
```

The blue lines tend to sort of flat for a little while to the right of the
one-year mark (52 weeks, along the x-axis). Also, one year is a pretty number.
Thus, I chose one year as the interval for this updatedness metric.

## Updatedness by portal
Now that we have this updatedness metric, let's see what it tells us.
Recall that the metric is the proportion of datasets older than a year that
have been updated within the year. Oldness is determined based on the
`publicationDate`, and last update is determined based on `viewLastModified`.

Keep in mind that there are lots of problems with the data I'm using.
First, I don't really know what `viewLastModified` means.
Second, there are problems with the way I am deduplicating the data.
Still, this will give you an idea of what metrics like this could tell us.

### How up-to-date are the portals?
Our updatedness measure is a number between 0 and 1, where 0 means that no
datasets are up-to-date and 1 means that all datasets are up-to-date. Let's
see what the updatedness measures are by portal.

```{r robustness_big_portals}
p5
```

It's harder to keep data up-to-date when you have more data, so let's plot
updatedness as a function of number of datasets.

```{r robustness_big_portals}
p6
```

As we should expect, portals with more datasets tend to be less up-to-date.
That is, there are very few portals in the top-right of the plot.

### What this tells us that counts don't
If you looked at just the dataset count metric, you'd think that
Kenya (`opendata.go.ke`) and King County are doing great jobs;
they both have a bit over 200 datasets.
On the other hand, if you look just at this updatedness metric,
you'd think that they're doing terrible jobs; only about 5% of
their datasets that are older than a year have been updated within the year.

I also find it interesting to compare the updatedness of different
portals with similar counts. For example, New York and Chicago are
the two highest in terms of dataset count. On the other hand,
New York scores much higher on our updatedness metric.
These two portals would appear similar if we just look at the count
metric, but they seem quite different when we look at both.

Given the problems with the data, I don't really trust these specific
conclusions, but they do point out that we could get a better picture
of our open data efforts by measuring more aspects of our performance.

## The bigger picture
