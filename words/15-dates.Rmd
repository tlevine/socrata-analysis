---
title: How up-to-date are data portal datasets?
tags: ['open-data']
---
```{r configure, echo = F}
source('15-dates.r')
opts_chunk$set(fig.width=10, fig.height=10, dpi = 42 * 5)
```

```{r up_to_date}
p1
```

## Data portal metrics
According to A, B, C and D, getting data to the web, in whatever form
you can, is always better than nothing. How can you tell whether you're
doing a good job at this basic release of data? Data catalog software
typically provides one tool: the metric of number of datasets. We can go to
[data.gov.uk](https://data.gov.uk) and find out that it currently has
however many datasets, and we can go back next week and make sure that
the number is increasing. There are problems with this metric, which I
won't go into right now, but the metric does tell us something about how
much data we are releasing.

According to A, B, C and D, it is also important that we keep open data
up-to-date. Unfortunately, data catalog software don't currently
have great tools for helping you make sure that your data are indeed
up-to-date. Specifically, we don't have a number that we can look at
week-to-week to make sure that we are vaguely on track at keeping data
up-to-date. So I made one.

## Developing the metric

### Motivation
In producing a metric of updateness, we are trying to come up with a
simple number that matches our intuition about whether something is
up-to-date. Let's think a bit about what would count as up-to-date.



### Differences by dataset
If a dataset of scores on standardized math tests taken in schools
was uploaded two weeks ago and hasn't been touched since, I'd say it's
up-to-date. A particular standardized math tests might be administered
yearly, so I don't expect the data to have changed in a couple of weeks.

On the other hand, if a dataset of 311 calls hasn't been updated in
two weeks, we could say that it is out-of-date because 311 calls are
always coming in.

A dataset about something that happens often needs to be refreshed
more often for us to consider it up-to-date. A very cool metric of
updatedness would account for out how often new data come in and
check whether those new data show up in the dataset. But that would
be a lot of work, so let's start with something simpler.

### Choosing the specific value
Instead of accounting for differences in datasets and in data portals,
I'm just going to choose one (somewhat arbitrary) value for this cutoff.
Instead of coming up with a less arbitrary cutoff, we can check whether
different values will give us the same value on our updatedness metric.
To say it fancy-like, we can check whether our updatedness metric is
robust to the cutoff.

A simple way of checking this is to plot the value of our updatedness
metric for different cutoff values. If we find a region where the value
of the updatedness metric doesn't change very much when we change the
cutoff, we could say that cutoffs in that region are safe to use.

```{r robustness}
p4
```

## Updatedness by portal

### 



### Rivalries

Chicago v new york



## Issues with this present analysis
federation and non-tables removed. but that doesn't work. i've explained this somewhere
