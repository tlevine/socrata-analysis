---
title: How up-to-date is your data catalog?
description: I looked at data from a bunch of open data catalogs, and it turns out that pretty much none of the datasets I was looking at have ever been updated.
tags: ['open-data']
created_at: 2013-11-22
kind: article
tweet_text: I’d assumed that people actually update their data, but it looks like people don’t.
twitter_image: figure/small_samples.png
facebook_image: figure/small_samples.png
other_source: https://github.com/tlevine/socrata-analysis/tree/master/words/15-dates.Rmd
---
```{r configure, echo = F}
source('15-dates.r')
opts_chunk$set(fig.width=10, fig.height=10, dpi = 42 * 5)
```

## Data portal statistics
According to A, B, C and D, it is important that we keep open data
up-to-date. Do we keep them up-to-date?

## The data about data
I [downloaded](/!/socrata-summary/)
all the metadata about all of the public datasets, and then I removed
datasets that were [duplicated because of federation](/!/socrata-deduplicate/#dealing-with-federation).

Socrata has this feature where you can create multiple
[views](/!/socrata-genealogies#term-view) from a single
[table](/!/socrata-genealogies#term-table).
It is hard to separate the official views, created by someone in the
government, from the unofficial, derived views. I did not try to determine
which view/dataset was the original, official one, but I did separate
them based on their `tableId`, which links all of the different views
that use the same data.

The Socrata API provides four date fields.
* `createdAt`
* `publicationDate`
* `rowsUpdatedAt`
* `viewLastModified`

`createdAt` is when someone clicked the button to include the dataset;
it's always the oldest one. It is possible to create a dataset but keep
it private, for internal review. If you spend a couple days reviewing
the dataset privately before you post it publically, the `publicationDate`
will reflect this latter date, while `createdAt` will still be the time
when you added the private dataset. Also, `createdAt` is different for
different views of the same data, but `publicationDate` is not.

```{r publicationDate}
table(subset(socrata.deduplicated, tableId == 926641)$publicationDate)
```

```{r createdAt}
table(subset(socrata.deduplicated, tableId == 926641)$createdAt)
```

`rowsUpdatedAt` and `viewLastModified` presumably have something to do
with the time the dataset was updated. I don't really know what those
mean, but I think `rowsUpdatedAt` is the one I want. Recall from above
that views with the same `tableId` share the share the underlying data.
If the data get updated, it should be reflected in all of the datasets.
This is the case for `rowsUpdatedAt`; for all
`r sum(socrata.deduplicated.orig$tableId == 926641)`
datasets with an `tableId` of `926641`, the `rowsUpdatedAt` field
is the same.

```{r rowsUpdatedAt}
table(subset(socrata.deduplicated.orig, tableId == 926641)$rowsUpdatedAt)
```

On the other hand, the `viewLastModified` field is quite different.

```{r viewLastModified}
table(subset(socrata.deduplicated.orig, tableId == 926641)$viewLastModified)
```

Thus, I'm using `rowsUpdatedAt` for calculating the updatedness measure.

## What is an "update"
I'm going to use the term "update"{:#term-update} to mean a reasonably particular
thing that excludes some other things that you might consider an update.

The Socrata data portal software has 
[this publisher API](http://dev.socrata.com/publishers/getting-started)
that lets the publisher (usually a government) upload data as a
[dataset view](/!/socrata-genealogies#term-view),
with its own unique URL based on its
[4x4 identifier](http://dev.socrata.com/docs/endpoints).

### Upload a new version and delete the old version
That's great for uploading a new dataset, but what do you do if a dataset
changes? For example, let's say you have a dataset of
[lottery contracts for 2011](https://data.oregon.gov/dataset/Contracts-Lottery-Fiscal-Year-2011/dwi6-thje)
and you notice a spelling error in one of the fields. You fix it on your own computer,
but how do you get it to the main site? You could upload the new one and delete the old
one, but then the URL will change.

When someone uploads a new version of the dataset to Socrata
but it has a different URL, I do not count it as an update.

### Upload new data as a separate dataset
Or what if you want to add the 2012 data?
You could upload the 2012 data as a
[separate dataset](https://data.oregon.gov/Revenue-Expense/Contracts-Lottery-Fiscal-Year-2012/i3ri-n6hq),
but then it would be hard to find; the open data catalog doesn't make a link between these two datasets.

Within the present article, I do not consider this to be an update either.

### Using the append and replace API methods
Not only can you create new datasets, you can also
[append to or replace](http://dev.socrata.com/publishers/importing#append_replace)
the data in an existing dataset. That is, you can upload new data for an
existing dataset while keeping its 4x4 identifier and all of its metadata.

This approach is much better than the other two because it
makes it much easier to find the new data; we can get the
new data while still referencing the same URL.

## Data summary
We have data from mid-2011 to mid-2013. In order to fit this all on
one plot, let's look first at which datasets were updated; we're ignoring
the dates when they were updated. Each horizontal band is a data portal,
each dot is a data [table](/!/socrata-genealogies#term-table)
inside of a data portal,
and the dots are colored based on whether the data were ever updated.

```{r summary}
p17
```

This plot shows us that older datasets are more likely to have been
updated. It also subtly points out that different data portals have
different amounts of datasets and that they have grown at different
rates over time. Let's plot that more clearly.

```{r dataset_counts_over_time}
p18
```

Data portals grow over time, often in sudden bursts. Two notes about
that plot:

1. It assumes that no data have ever been deleted from a portal;
    it's all based on the publication date.
2. It is counting the number of [tables](/!/socrata-genealogies#term-table)
    rather than the number of [views](/!/socrata-genealogies#term-view).

Let's also relate the update dates to the publication dates.

```{r up_to_date}
p1
```

It looks like most data were "updated" only at their time of their
initial publication. Colloquially, we'd say that they haven't been
updated ever. But there are some spikes here and there.

Also, it appears that only certain kinds of data get updated.

```{r viewType_table}
table(socrata.deduplicated$viewType, is.na(socrata.deduplicated$rowsUpdatedAt))
```

## Motivation for the measure
This is a bit complicated; let's try to come up with simpler measure
for taking this in.

One measure that I came up with is **proportion of datasets older than
a year that have been updated within the year**. (The timespan of a
year is pretty arbitrary, though.) Here's how I arrived at that measure.

### General formulation
In producing a measure of updateness, we are trying to come up with a
simple number that matches our intuition about whether something is
up-to-date. Let's think a bit about what would count as up-to-date.

A dataset first gets published at some time. For a little while, the
data will be up-to-date even if no updates are sent. For example, if
data about wifi usage per month are uploaded on the first of the month,
we'll consider them to be up-to-date two weeks later.

After some point, we must have received an update in order for the
data to be up-to-date. In the case of monthly wifi usage, we need an
update on the first of the second month; if we haven't received one
by then, we'll consider the data out-of-date. Thus, I arrive at the
general concept of comparing the date of initial publication to the
date at which the dataset was last updated.

### Differences by dataset
If a dataset of scores on standardized math tests taken in schools
was uploaded two weeks ago and hasn't been touched since, I'd say it's
up-to-date. A particular standardized math tests might be administered
yearly, so I don't expect the data to have changed in a couple of weeks.

On the other hand, if a dataset of 311 calls hasn't been updated in
two weeks, we could say that it is out-of-date because 311 calls are
always coming in.

A dataset about something that happens often needs to be refreshed
more often for us to consider it up-to-date. A very cool measure of
updatedness would account for out how often new data come in and
check whether those new data show up in the dataset. But that would
be a lot of work, so let's start with something simpler.

### Keep it simple
Rather than worring about the exact time period, let's address a simpler
issue. I've heard complaints that some data get uploaded data once and
never updated, ever. Rather than worring about the exact time since update,
let's just check whether the dataset has been updated ever.

I'm going to use the format of *datasets older than X that have been updated since*
and just choose an appropriate time X. The time probably has to be kind of
long, so that we don't count datasets as out-of-date when they just have
long update cycles (like yearly data). On the other hand, the time can't be
too long, because then we'll wind up saying that everything is out-of-date.

### Choosing a less arbitrary cutoff
We can check whether different values will give us the same value on our
updatedness measure. To say it fancy-like, we can check whether our updatedness
measure is robust to the cutoff.

A simple way of checking this is to plot the value of our updatedness
measure for different cutoff values. If we find a region where the value
of the updatedness measure doesn't change very much when we change the
cutoff, we could say that cutoffs in that region are safe to use.

I also had to decide whether to use `rowsUpdatedAt` or `viewLastModified`
as the date of update. I don't really know what either of these means, so
let's just try both.

```{r robustness}
p3
```

The main thing I notice is that the value of this statistic
(again, proportion of datasets older than some date that have been updated since)
is pretty close to zero for all of the portals at any time.
This is very bad; it means that we pretty much never update
data that we put on data portals.

The next thing I notice is that the value varies widely when there aren't
very many datasets on the portal. Visually, this shows itself as skinny
lines (few datasets) jumping up and down very suddenly, like for
`data.cms.gov`. When there are very few datasets on a portal
(like 12), an addition of a dataset or an update to a dataset will have
a large impact on the value of our updatedness measure (the height of the line).
This is not interesting, and the plot below should explain why.

```{r small_samples}
p4()
```

In these two plots, we're only looking at `data.cms.gov`. The first
plot is the same as the corresponding plot in the previous image,
only larger. The second shows the dates at which each
[data table](/!/socrata-genealogies#term-table) was
published and most recently updated. Red dots are publication dates,
and blue dots are update dates. If there was no update or the most
recent update was within a day of the publication date, no blue dot
is shown.

The bumps in the top plot only occur when the portal contains very
few datasets; the figure stabilizes at the end of March 2013, when
a bunch of datasets get published. (In case you're curious, these
datasets are all derived from the
[Beneficiary Summary File](http://www.resdac.org/cms-data/files/bsf),
according to their respective descriptions in the portal.)

## Even simpler
I'd assumed that people actually update their data, but it looks
like people don't.
I think we need something simpler to drive this conclusion home.

Let's look at the number of datasets that have ever been updated, by
portal. We'll count a dataset as having been updated if it has a value
in `rowsUpdatedAt` that is more than one day greater than the value
in `publicationDate`.


```{r any_update}
p5
```

Keep my [definition of "update"](#term-update) in mind;
in the above count, I'm including neither situations where old datasets
were deleted and replaced with new ones nor situations where new records
were uploaded as a separate dataset.

### Which datasets have been updated?
Only `r sum(socrata.deduplicated$has.been.updated)` datasets have
ever been updated. Which ones are they?

These `r sum(socrata.deduplicated$has.been.updated)` datasets are
contained within only `r length(unique(subset(socrata.deduplicated, has.been.updated)$portal))`
portals, out of the `r length(unique(socrata.deduplicated$portal))`
that I looked at. Here are the updates over time by portal.

```{r updates_over_time_by_portal}
p6
```

A few datasets are still updated today (indicated by the tiny bumps
towards the right of the graphs), but most were only updated two years
ago.

You might have guessed already, but these recent updates tend to be for
datasets that were recently updated, rather than old datasets that have
been maintained for a while. You can see that in this plot of publication
date versus update date. The update date is along the x-axis, and the
publication date is along the y-axis. The diagonal line is where the
publication date and update date are the same, and it is impossible for
a dataset to appear above the diagonal line. Datasets towards the bottom-left
of the plot were published and updated a long time ago. Datasets towards
the top-right were published and updated recently. Datasets towards the
bottom-right were published a long time ago and updated recently.

```{r publish_v_update}
p10
```

I didn't really look into why there were so many updates a long
time ago and so few in between, but I suspect it has something to do
with how all of these portals are managed by Socrata; there might have
been some change in software or in technical support practices that
would have impacted all of the portals.

### Which old datasets are still kept up-to-date?
Recall that the points at the bottom-right of the previous plot
correspond to old datasets that have been updated recently. Let's
look more closely at these.

I selected the datasets that were uploaded before 2013 and have
been updated during 2013; they are represented the ones contained
by the rectangle in the plot below.

```{r publish_v_update_2013}
p11
```

There are, in fact, only `r nrow(updates.2013)` such datasets,
and they're in `r length(unique(updates.2013$portal))` portals.

```{r updates_2013_hist}
p12
```

Here they are by url.

```{r updates_2013_url}
p13
```

I was wondering whether anything was special about these. Maybe
they have a lot of records or get downloaded a lot? For both of
these statistics, I'm using the total across all [views](/!/socrata-genealogies#term-view) in the
[table/family](/!/socrata-genealogies#term-table), not just the value for that particular view.

```{r updates_2013_specialness}
p16
```

Not really. You can try to find specialness in them, though;
here they are.

```{r updates_2013_list}
cat(paste(
  paste0(
    '* `', updates.2013.joined$portal, '`: [', updates.2013.joined$name, ']',
    '(https://', updates.2013.joined$portal, '/-/-/', updates.2013.joined$id, ')',
  ),
  collapse = '\n'
))
```

Note well: These links go to arbitrary [views](/!/socrata-genealogies#term-view) on the particular
[table](/!/socrata-genealogies#term-table) that was updated; you'll have to follow a few links to get to
the original dataset.

### Updated datasets in context
Maybe there actually is something different about the sort of dataset that
gets updated.

It turns out that data that have been updated tend to get more downloads than
data that data that haven't. (This again uses the family/table totals rather
that the values for the particular views.)

```{r update_download}
p14
```

To give you some more concrete numbers, the median download count was
`r m14.medians[1]` among data tables that got updated and `r m14.medians[2]`
among data tables that didn't. A Wilcoxon rank sum test says that this
difference is significant (assuming that our `r nrow(socrata.deduplicated)`
datasets are a representative sample of some super-population, yadda yadda),
with a p-value far less than 0.001.

There is a similar relationship with number of records.
It turns out that data that have been updated tend to contain more records than
data that data that haven't.

```{r update_nrow}
p15
```

The median record count was
`r m15.medians[1]` among data tables that got updated and `r m15.medians[2]`
among data tables that didn't. A Wilcoxon rank sum test says that this
difference too is significant.

## A proposal for an updatedness measure
Having found hardly any datasets that get updated, it's difficult for me to
say whether any measure I come up with will be all that helpful. But here's
my best guess as to the measure we should try to arrive at.

### Update cycles
As I explained above, different datasets have different update cycles
(daily, weekly, quarterly, yearly, &c.), and we can know what this cycle
is for most datasets. In many cases, we don't know exactly how often a
dataset will be updated, but we can be reasonably confident that it will
be within a certain range.

### What we do in treasury.io
We do something like this in [treasury.io](http://treasury.io).
The Financial Management Service (FMS) provides a daily treasury
statement. One statement is provided per work day, so we should
get updates approximately every day. In order to make sure that
our daily importer is working, we check every day that the
resulting dataset is up-to-date.

We know from experience that
the file doesn't always come out on time; that is, we might only recieve
today's statement two days from now. On the other hand, we've
rarely (never?) seen it take longer than that. Our daily updatedness
test checks how far behind the data are, and it sends us an email
if the data are
[more than three days behind](https://github.com/csvsoundsystem/federal-treasury-api/blob/master/tests/is_it_running.py#L47);
if this happens,
we'll suspect that it's something wrong with our importer rather
than just a late upload from the FMS.

### Possibilities
If we indicate how often each dataset needs to
be updated, the data portal software could make us aware of which datasets
need updating.

<!-- mock-up of a fancy interface -->

We could also see how the updatedness of group of datasets compares
with that of another group.

<!-- mock-up of a fancy interface -->

## The bigger picture
I thought this was going to find out which datasets are more up-to-date,
when they get updated, and so on, but I really just found that the data
are completely out-of-date.

Perhaps this says something about the need for more measures of the progress of
our open data efforts. We have a crude measure of the size of a data catalog
(the number of datasets it contains), so we can check that number and make sure
that it is increasing. But that's pretty much the only one we have, and there
are surely other things that are worth measuring. Nobody is measuring updatedness,
so how can we expect anyone to know that the data are out-of-date?
