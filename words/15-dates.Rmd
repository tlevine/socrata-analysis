---
title: 
tags: ['open-data']
---
```{r configure, echo = F}
source('15-dates.r')
opts_chunk$set(fig.width=10, fig.height=10, dpi = 42 * 5)
```

## Data portal metrics
According to A, B, C and D, getting data to the web, in whatever form
you can, is always better than nothing. How can you tell whether you're
doing a good job at this basic release of data? Data catalog software
typically provides one tool: the metric of number of datasets. We can go to
[data.gov.uk](https://data.gov.uk) and find out that it currently has
however many datasets, and we can go back next week and make sure that
the number is increasing. There are problems with this metric, which I
won't go into right now, but the metric does tell us something about how
much data we are releasing.

According to A, B, C and D, it is also important that we keep open data
up-to-date. Unfortunately, data catalog software don't currently
have great tools for helping you make sure that your data are indeed
up-to-date. Specifically, we don't have a number that we can look at
week-to-week to make sure that we are vaguely on track at keeping data
up-to-date. So I made one.

## The data about data
I [downloaded]()
all the metadata about all of the public datasets, and then I removed
datasets that were [duplicated because of federation]().

Socrata has this feature where you can create multiple
[views]() from a single
[table]().
It is hard to separate the official views, created by someone in the
government, from the unofficial, derived views. I did not try to determine
which view/dataset was the original, official one, but I did separate
them based on their `tableId`, which links all of the different views
that use the same data.

The Socrata API provides four date fields.
* `createdAt`
* `publicationDate`
* `rowsUpdatedAt`
* `viewLastModified`

`createdAt` is when someone clicked the button to include the dataset;
it's always the oldest one. It is possible to create a dataset but keep
it private, for internal review. If you spend a couple days reviewing
the dataset privately before you post it publically, the `publicationDate`
will reflect this latter date, while `createdAt` will still be the time
when you added the private dataset. Also, `createdAt` is different for
different views of the same data, but `publicationDate` is not.

```{r publicationDate}
table(subset(socrata.deduplicated, tableId == 926641)$publicationDate)
```

```{r createdAt}
table(subset(socrata.deduplicated, tableId == 926641)$createdAt)
```

`rowsUpdatedAt` and `viewLastModified` presumably have something to do
with the time the dataset was updated. I don't really know what those
mean, but I think `rowsUpdatedAt` is the one I want. Recall from above
that views with the same `tableId` share the share the underlying data.
If the data get updated, it should be reflected in all of the datasets.
This is the case for `rowsUpdatedAt`; for all
`r sum(socrata.deduplicated.orig$tableId == 926641)`
datasets with an `tableId` of `926641`, the `rowsUpdatedAt` field
is the same.

```{r rowsUpdatedAt}
table(subset(socrata.deduplicated.orig, tableId == 926641)$rowsUpdatedAt)
```

On the other hand, the `viewLastModified` field is quite different.

```{r viewLastModified}
table(subset(socrata.deduplicated.orig, tableId == 926641)$viewLastModified)
```

Thus, I'm using `rowsUpdatedAt` for calculating the updatedness measure.

## Data summary
Let's start by plotting these date variables by dataset.
I also included the publication group number, which is presumably
an identification number that increases with each group of dataset
publication.

```{r up_to_date}
p1
```

It looks like most data were "updated" only at their time of their
initial publication. Colloquially, we'd say that they haven't been
updated ever. But there are some spikes here and there.

This is a bit complicated; let's try to come up with simpler measure
for taking this in.

## Motivation for the metric
The metric that I came up with is **proportion of datasets older than
a year that have been updated within the year**. Here's how I arrived
at that metric.

### General formulation
In producing a metric of updateness, we are trying to come up with a
simple number that matches our intuition about whether something is
up-to-date. Let's think a bit about what would count as up-to-date.

A dataset first gets published at some time. For a little while, the
data will be up-to-date even if no updates are sent. For example, if
data about wifi usage per month are uploaded on the first of the month,
we'll consider them to be up-to-date two weeks later.

After some point, we must have received an update in order for the
data to be up-to-date. In the case of monthly wifi usage, we need an
update on the first of the second month; if we haven't received one
by then, we'll consider the data out-of-date. Thus, I arrive at the
general concept of comparing the date of initial publication to the
date at which the dataset was last updated.

### Differences by dataset
If a dataset of scores on standardized math tests taken in schools
was uploaded two weeks ago and hasn't been touched since, I'd say it's
up-to-date. A particular standardized math tests might be administered
yearly, so I don't expect the data to have changed in a couple of weeks.

On the other hand, if a dataset of 311 calls hasn't been updated in
two weeks, we could say that it is out-of-date because 311 calls are
always coming in.

A dataset about something that happens often needs to be refreshed
more often for us to consider it up-to-date. A very cool metric of
updatedness would account for out how often new data come in and
check whether those new data show up in the dataset. But that would
be a lot of work, so let's start with something simpler.

### All or nothing
Rather than worring about the exact time period, let's address a simpler
issue. I've heard complaints that some data get uploaded data once and
never updated, ever. Rather than worring about the exact time since update,
let's just check whether the dataset has been updated ever.

I'm going to use the format of *datasets older than X that have been updated since*
and just choose an appropriate time X. The time probably has to be kind of
long, so that we don't count datasets as out-of-date when they just have
long update cycles (like yearly data). On the other hand, the time can't be
too long, because then we'll wind up saying that everything is out-of-date.

### Choosing the specific value
We can check whether different values will give us the same value on our
updatedness metric. To say it fancy-like, we can check whether our updatedness
metric is robust to the cutoff.

A simple way of checking this is to plot the value of our updatedness
metric for different cutoff values. If we find a region where the value
of the updatedness metric doesn't change very much when we change the
cutoff, we could say that cutoffs in that region are safe to use.

I also had to decide whether to use `rowsUpdatedAt` or `viewLastModified`
as the date of update. I don't really know what either of these means, so
let's just try both.

```{r robustness}
p4
```

The first thing I notice when I look at this plot is that the metric based
on `rowsUpdatedAt` (labeled "rows") is pretty much always zero.
If `rowsUpdatedAt` means what I intuitively think of as the date of an update
to the data, this is very bad; it means that we pretty much never update
data that we put on data portals.

The next thing I notice is that the value varies widely when there aren't
very many datasets on the portal. Visually, this shows itself as skinny
lines (few datasets) jumping up and down very suddenly, like for
`nmfs.socrata.com`. When there are very few datasets on a portal
(like 12), an addition of a dataset or an update to a dataset will have
a large impact on the value of our updatedness metric (the height of the line).

## Something simpler
I'd assumed that people actually update their data, but it looks like
nobody keeps their data up-to-date except Seattle and maybe New York.
I think we need something simpler to drive this conclusion home.






### How up-to-date are the portals?
Our updatedness measure is a number between 0 and 1, where 0 means that no
datasets are up-to-date and 1 means that all datasets are up-to-date. Let's
see what the updatedness measures are by portal.

```{r robustness_big_portals}
p5
```

It's harder to keep data up-to-date when you have more data, so let's plot
updatedness as a function of number of datasets.

```{r robustness_big_portals}
p6
```

As we should expect, portals with more datasets tend to be less up-to-date.
That is, there are very few portals in the top-right of the plot.

### What this tells us that counts don't
If you looked at just the dataset count metric, you'd think that
Kenya (`opendata.go.ke`) and King County are doing great jobs;
they both have a bit over 200 datasets.
On the other hand, if you look just at this updatedness metric,
you'd think that they're doing terrible jobs; only about 5% of
their datasets that are older than a year have been updated within the year.

I also find it interesting to compare the updatedness of different
portals with similar counts. For example, New York and Chicago are
the two highest in terms of dataset count. On the other hand,
New York scores much higher on our updatedness metric.
These two portals would appear similar if we just look at the count
metric, but they seem quite different when we look at both.

Given the problems with the data, I don't really trust these specific
conclusions, but they do point out that we could get a better picture
of our open data efforts by measuring more aspects of our performance.

## The bigger picture



Local law 11
March 18
September something
